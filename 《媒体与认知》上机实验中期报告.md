# 《媒体与认知》上机实验中期报告

## 一、整体方案理解

本次实验旨在实现一个简化版本的 CLIP 模型，通过图像与文本的对比学习，实现多模态语义对齐，并支持文本与图像之间的相互检索。核心思想是将图像和文本分别编码为向量，并通过 InfoNCE 损失函数在训练阶段拉近正样本对之间的距离，推远负样本对的距离，从而使得模型在共享语义空间中能够理解“图”和“文”的对应关系。

具体实现采用双塔结构，图像编码器使用 ResNet18 进行图像特征提取并投影到嵌入空间，文本编码器使用双层 LSTM 网络提取句子语义信息，后接一个线性层对齐维度。整个系统的输出为图文嵌入对，使用余弦相似度作为匹配依据，通过双向 InfoNCE 损失函数进行训练。

------

## 二、实验过程描述

### 图像编码器（Task1）

图像编码器基于预训练的 ResNet18 网络，移除原有全连接层，仅保留卷积层用于提取特征。输出的 feature map 经全局平均池化后，通过一个线性层映射到 256 维的共享语义空间，并进行归一化处理。

```python
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        # 实现第一个 3x3 卷积层，包含 stride、padding=1，bias=False；
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        # 【填空区域】：对 conv1 的输出接 BatchNorm
        self.bn1 = nn.BatchNorm2d(out_channels)

        # 定义 ReLU 激活，inplace 设为 True
        self.relu = nn.ReLU(inplace=True)
        # 【填空区域】：实现第二个 3x3 卷积层，步长默认为1
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)

        # conv2 的输出接 BatchNorm
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.downsample = downsample

    # 请完成forward函数
    def forward(self, x):
        identity = x  # 保存输入用于残差连接

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)  # 匹配维度

        out += identity  # 残差连接
        out = self.relu(out)
        return ou
```

### 文本编码器（Task2）

文本编码器使用双层 LSTM 网络处理文本序列，提取每句话的语义表示。LSTM 的最后一个隐藏状态作为文本表示，经过线性层映射并归一化。

```python
    def forward(self, captions: torch.Tensor):
        """
        captions: [B, T]，表示批次 B 中每个句子的 token id 序列，T 为序列长度。
        """

        B, T = captions.size()
        x = self.embedding(captions)  # [B, T, embed_dim]

        # 初始化输出隐状态和细胞状态
        h = torch.zeros(B, self.hidden_dim, device=captions.device, dtype=x.dtype)
        c = torch.zeros(B, self.hidden_dim, device=captions.device, dtype=x.dtype)

        # 生成mask，忽略padding_idx的token
        mask = (captions != self.embedding.padding_idx)  # [B, T]

        for t in range(T):
            x_t = x[:, t, :]  # 当前时间步的输入 [B, embed_dim]

            g_t = torch.tanh(self.W_c(x_t) + self.U_c(h))     # 输入节点
            i_t = torch.sigmoid(self.W_i(x_t) + self.U_i(h))  # 输入门
            f_t = torch.sigmoid(self.W_f(x_t) + self.U_f(h))  # 遗忘门
            o_t = torch.sigmoid(self.W_o(x_t) + self.U_o(h))  # 输出门

            # 仅更新非padding位置
            mask_t = mask[:, t].unsqueeze(1).type_as(h)  # [B, 1]
            c = i_t * g_t + f_t * c
            c = mask_t * c + (1 - mask_t) * c.detach()
            h = o_t * torch.tanh(c)
            h = mask_t * h + (1 - mask_t) * h.detach()

        # 将最终隐状态 h 通过全连接层映射至目标嵌入空间
        out = self.fc(h) # [B, embed_dim]
        # 对输出结果进行 L2 正则化归一化，确保嵌入向量单位化
        return F.normalize(out, p=2, dim=1)
```

### 对比损失函数（Task3）

实现了双向 InfoNCE 损失函数，包括图像到文本和文本到图像两个方向，具体如下：

```python
def contrastive_loss(image_embeds: torch.Tensor, text_embeds: torch.Tensor, temperature=0.07):
    """
    image_embeds, text_embeds: [batch, embed_dim]
    """

    image_embeds = F.normalize(image_embeds, p=2, dim=1)
    text_embeds = F.normalize(text_embeds, p=2, dim=1)

    # 余弦相似度矩阵：[B, B]
    logits = image_embeds @ text_embeds.T / temperature  # 图像作为query
    logits = logits.float()  # Ensure logits are float for cross_entropy
    labels = torch.arange(image_embeds.size(0), device=image_embeds.device)
    l_i2t = F.cross_entropy(logits, labels)
    l_t2i = F.cross_entropy(logits.T, labels)

    return 0.5 * (l_i2t + l_t2i)
```

------

## 三、实验结果分析

在 Flickr8k 数据集上进行了训练和评估，主要观测指标包括损失函数下降曲线、Top-1 Accuracy 以及 Recall@K 检索准确率。

### 训练曲线

下图展示了训练集上的损失下降趋势：

<img src="C:\Users\AdorN\OneDrive - mails.tsinghua.edu.cn\桌面\a4ea8d56-5428-4417-9ec1-4636cdddff92.jpg" alt="a4ea8d56-5428-4417-9ec1-4636cdddff92" style="zoom: 25%;" />

可以看出模型在前几轮收敛较快，之后趋于平稳，表明模型有效学习了图文之间的对应关系。

### 检索性能

<img src="C:\Users\AdorN\AppData\Roaming\Typora\typora-user-images\image-20250516235157107.png" alt="image-20250516235157107" style="zoom:50%;" />

------

## 四、可视化展示

实现了文本检索图像（Text → Image）的可视化，展示模型对于指定文本返回的 Top-5 图像如下：

<img src="C:\Users\AdorN\AppData\Roaming\Typora\typora-user-images\image-20250516235448531.png" alt="image-20250516235448531" style="zoom: 33%;" />

------

## 五、总结与反思

本次实验成功实现了一个简化版的 CLIP 模型，完成了图像与文本嵌入空间对齐的基本目标。在实验过程中，我掌握了对比学习的核心思想、双塔结构的建模方式以及 InfoNCE 损失函数的使用方法。

模型在简单场景下检索效果良好，但在复杂语义表达下仍存在一定误差。可能的改进方向包括：

- 使用更强大的预训练文本模型（如 BERT）增强文本理解能力
- 替换图像编码器为 ViT 等视觉 Transformer 结构
- 引入数据增强策略（如文本同义替换、图像色彩扰动）提高模型泛化能力
- 尝试多任务学习，如图文分类辅助任务